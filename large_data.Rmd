---
title: "Notes from Akande's Working iwth Large Data"
output:
  html_document:
    df_print: paged
    self_contained: false
    toc: yes
    toc_float: yes
    highlight: tango
---

DataFest: Large Data in R

2018-03-29

- [michael akande](https://github.com/akandelanre)
- http://www.stat.duke.edu/~oma9/datafest/df2018
- http://www2.stat.duke.edu/~oma9/datafest/df2018/data/ 

# Data

660 MB file:  [2008.csv](https://gitlab.oit.duke.edu/john/tutotial-big-data-large-data/blob/master/data/2008.csv)

## Package Libraries

- *reading in the data*

    - readr (tidyverse -- better than read.csv) 
    - data.table (competitive advantage when reading data larger than 250 Mb)
    
- data manipulators

    - dplyr
    - ggplot2

 
- pryr              

- Benchmarking

    - microbenchmark
    - benchmark
    - profvis

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(data.table)
```

```{}
file.info(list.files("data", full.names=TRUE))

```



## Load Data

### Comparing times

| Function | user | system | elapsed |
| ---------|-----:|-------:|--------:|
readr::read_csv | 94.01 | 84.73 | 181.98 
data.table::fread | 6.78 | 0.69 | 2.72 

### read_csv

standard read with `read_csv`

```{r message=FALSE, warning=FALSE, include=FALSE}
air <- read_csv("../tutotial-big-data-large-data/data/2008.csv")
```

``` r
air <- read_csv("data/2008.csv")
```

## data.table

Much faster with `fread()`!

The data.table package is useful for importing data larger than 1-2 GB.  It's often a good idea to convert the imported data frame to a tibble:  `my_tibble <- as_tibble(my_imported_data.table)`

```{r}
system.time({flights <- fread("../tutotial-big-data-large-data/data/2008.csv",
                             showProgress = FALSE)})

#class(flights) # "data.frame"

flights2 <- as_tibble(flights)
```






``` r
system.time({flights3 <- fread("data/2008.csv", showProgress = FALSE)})
# user    system    elapsed 
# 6.78    0.69      2.72 
system.time({air2 <- read_csv("data/2008.csv")})
#    user  system  elapsed 
#   94.01   84.73  181.98 
```



## Preserve data table as a binary matrix

**.Rdata** 

does note appear to load faster than even `fread()`

```{r}
save(flights, file="data/flights.Rdata")
system.time(load(file="data/flights.Rdata"))
```

## Practical Advice

Use **databases** when the data are much larger than 1-2 GB 

look at the DB tool in tidyverse:  `library(dbplyr`.  https://dbplyr.tidyverse.org/  

## Sampling the data

use `sample_frac()`

```{r}
library(pryr)
```

flights subset is a sample of flights

```{r}
set.seed(20180329)
flights_sub = flights %>% sample_frac(0.2)
```


```{r}
object.size(flights)
object.size(flights_sub)
```

## then free up memory

```{r}
rm(flights)
rm(air)
gc()  # garbage collection
```

## Other Useful Tools

### Progress Bars

```{r}
p <- progress_estimated(50, min_time = 0)
for(i in 1:50)
{
  # Calculate something compliated
  Sys.sleep(0.1)
  
  p$tick()$print()
}
```

## BenchMarking

```{r}
system.time(rnorm(1e6))
system.time(rnorm(1e4) %*% t(rnorm(1e4)))
```

# Benchmark

`library(microbenchmark)`

Or, We can also use the rbenchmark package  `library(rbenchmark)`


```{r}
library(microbenchmark)

d <- abs(rnorm(1000))
r <- microbenchmark(
      exp(log(d)/2),
      d^0.5,
      sqrt(d),
      times = 1000
    )
print(r)
```

# Profiling

```{r}
library(profvis)

set.seed(20180329)
flights_small<- flights_sub %>% sample_n(100000)
profvis({
  m = lm(AirTime ~ Distance, data = flights_small)
  plot(AirTime ~ Distance, data = flights_small)
  abline(m, col = "red")
})
```

# PDF Graphics

```{r}
png("time_vs_dist.png", width=1024, height=800)
ggplot(flights_small, aes(y=AirTime,x=Distance)) +
  geom_point(alpha=0.01, size=0.5)
dev.off()
```


```{r}
ggsave("time_vs_dist_ggsave.png")
```

![](time_vs_dist_ggsave.png)